{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aeedd06",
   "metadata": {},
   "source": [
    "# nn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for PyTorch, math and array utilities\n",
    "# - torch: main deep learning framework\n",
    "# - nn: neural network building blocks (layers, modules)\n",
    "# - optim: optimization algorithms (SGD, Adam, etc.)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# random is used to create varied synthetic sequences\n",
    "import random\n",
    "\n",
    "# Import the Transformer model implementation from the local utils file\n",
    "from utils import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5e302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562 batches of size 16\n",
      "187 batches of size 16\n"
     ]
    }
   ],
   "source": [
    "# Data generation and batching helpers\n",
    "# - generate_random_data: creates synthetic sequence pairs (X, y) with SOS/EOS tokens\n",
    "# - batchify_data: groups sequences into fixed-size batches (optional padding not used here)\n",
    "def generate_random_data(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "    length = 8\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 0,0,0,0 -> 0,0,0,0\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1,0,1,0 -> 1,0,1,0,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.zeros(length)\n",
    "        start = random.randint(0, 1)\n",
    "\n",
    "        X[start::2] = 1\n",
    "\n",
    "        y = np.zeros(length)\n",
    "        if X[-1] == 0:\n",
    "            y[::2] = 1\n",
    "        else:\n",
    "            y[1::2] = 1\n",
    "\n",
    "        X = np.concatenate((SOS_token, X, EOS_token))\n",
    "        y = np.concatenate((SOS_token, y, EOS_token))\n",
    "\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_bath_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "\n",
    "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "train_data = generate_random_data(9000)\n",
    "val_data = generate_random_data(3000)\n",
    "\n",
    "train_dataloader = batchify_data(train_data)\n",
    "val_dataloader = batchify_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6df3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitingupta/usc/fa25/csce584/neural-network-applications/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Device selection and model instantiation\n",
    "# - choose CUDA if available, otherwise CPU\n",
    "# - create Transformer instance and move it to the selected device\n",
    "# - define optimizer (SGD) and the loss function (cross-entropy)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(\n",
    "    num_tokens=4,\n",
    "    dim_model=8,\n",
    "    num_heads=2,\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=3,\n",
    "    dropout_p=0.1,\n",
    ").to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for one epoch\n",
    "# - iterates batches, computes predictions, loss, and performs optimizer step\n",
    "# - uses the model's tgt mask to prevent peeking into future tokens\n",
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        X, y = batch[:, 0], batch[:, 1]\n",
    "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:, :-1]\n",
    "        y_expected = y[:, 1:]\n",
    "\n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        pred = pred.permute(1, 2, 0)\n",
    "        loss = loss_fn(pred, y_expected)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loop (evaluation mode, no gradient updates)\n",
    "# - computes average loss across validation batches\n",
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[:, 0], batch[:, 1]\n",
    "            X, y = (\n",
    "                torch.tensor(X, dtype=torch.long, device=device),\n",
    "                torch.tensor(y, dtype=torch.long, device=device),\n",
    "            )\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:, :-1]\n",
    "            y_expected = y[:, 1:]\n",
    "\n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            pred = pred.permute(1, 2, 0)\n",
    "            loss = loss_fn(pred, y_expected)\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating model\n",
      "------------------------- Epoch 1 -------------------------\n",
      "Training loss: 0.5615\n",
      "Validation loss: 0.4011\n",
      "\n",
      "------------------------- Epoch 2 -------------------------\n",
      "Training loss: 0.4093\n",
      "Validation loss: 0.3715\n",
      "\n",
      "------------------------- Epoch 3 -------------------------\n",
      "Training loss: 0.3804\n",
      "Validation loss: 0.3357\n",
      "\n",
      "------------------------- Epoch 4 -------------------------\n",
      "Training loss: 0.3477\n",
      "Validation loss: 0.2959\n",
      "\n",
      "------------------------- Epoch 5 -------------------------\n",
      "Training loss: 0.3170\n",
      "Validation loss: 0.2555\n",
      "\n",
      "------------------------- Epoch 6 -------------------------\n",
      "Training loss: 0.2965\n",
      "Validation loss: 0.2410\n",
      "\n",
      "------------------------- Epoch 7 -------------------------\n",
      "Training loss: 0.2806\n",
      "Validation loss: 0.2182\n",
      "\n",
      "------------------------- Epoch 8 -------------------------\n",
      "Training loss: 0.2695\n",
      "Validation loss: 0.2033\n",
      "\n",
      "------------------------- Epoch 9 -------------------------\n",
      "Training loss: 0.2585\n",
      "Validation loss: 0.1915\n",
      "\n",
      "------------------------- Epoch 10 -------------------------\n",
      "Training loss: 0.2493\n",
      "Validation loss: 0.1841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training driver: runs multiple epochs and records training/validation loss\n",
    "# - returns lists of training and validation losses for plotting or analysis\n",
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "\n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\" * 25, f\"Epoch {epoch + 1}\", \"-\" * 25)\n",
    "\n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "\n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "\n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "\n",
    "    return train_loss_list, validation_loss_list\n",
    "\n",
    "\n",
    "train_loss_list, validation_loss_list = fit(\n",
    "    model, opt, loss_fn, train_dataloader, val_dataloader, 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea772c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Continuation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Example 1\n",
      "Input: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Continuation: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Example 2\n",
      "Input: [1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Continuation: [1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "\n",
      "Example 3\n",
      "Input: [0, 1, 0, 1, 0, 1, 0, 1]\n",
      "Continuation: [1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "\n",
      "Example 4\n",
      "Input: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Continuation: [0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "\n",
      "Example 5\n",
      "Input: [0, 1]\n",
      "Continuation: [1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Greedy prediction helper: given a source sequence, generate a continuation\n",
    "# - starts with SOS token and repeatedly predicts next token until EOS or max length\n",
    "def predict(model, input_sequence, max_length=15, SOS_token=2, EOS_token=3):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "\n",
    "    num_tokens = len(input_sequence[0])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
    "\n",
    "        pred = model(input_sequence, y_input, tgt_mask)\n",
    "\n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item()  # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]], device=device)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()\n",
    "\n",
    "\n",
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 3]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 3]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[2, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 3]], dtype=torch.long, device=device),\n",
    "    torch.tensor(\n",
    "        [[2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device\n",
    "    ),\n",
    "    torch.tensor([[2, 0, 1, 3]], dtype=torch.long, device=device),\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-network-applications",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
