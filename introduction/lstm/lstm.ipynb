{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e06143",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c177fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar utility for loops (training), and numpy for arrays/ops\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions provided in the local `utils.py`:\n",
    "# - initWeights: initialize weight matrices with small random values\n",
    "# - sigmoid, tanh: activation functions (sigmoid supports derivative flag)\n",
    "# - softmax: convert raw logits to probability distributions\n",
    "from utils import initWeights, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ea371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 866, Char Size: 32\n"
     ]
    }
   ],
   "source": [
    "# Load a sample corpus (Hamlet soliloquy) and normalize to lowercase.\n",
    "# This small text will be used for character-level language modeling.\n",
    "data = \"\"\"To be, or not to be, that is the question: Whether \\\n",
    "'tis nobler in the mind to suffer The slings and arrows of ou\\\n",
    "trageous fortune, Or to take arms against a sea of troubles A\\\n",
    "nd by opposing end them. To die—to sleep, No more; and by a s\\\n",
    "leep to say we end The heart-ache and the thousand natural sh\\\n",
    "ocks That flesh is heir to: 'tis a consummation Devoutly to b\\\n",
    "e wish'd. To die, to sleep; To sleep, perchance to dream—ay, \\\n",
    "there's the rub: For in that sleep of death what dreams may c\\\n",
    "ome, When we have shuffled off this mortal coil, Must give us\\\n",
    " pause—there's the respect That makes calamity of so long lif\\\n",
    "e. For who would bear the whips and scorns of time, Th'oppres\\\n",
    "sor's wrong, the proud man's contumely, The pangs of dispriz'\\\n",
    "d love, the law's delay, The insolence of office, and the spu\\\n",
    "rns That patient merit of th'unworthy takes, When he himself \\\n",
    "might his quietus make\"\"\".lower()\n",
    "\n",
    "chars = set(data)\n",
    "\n",
    "data_size, char_size = len(data), len(chars)\n",
    "\n",
    "print(f\"Data size: {data_size}, Char Size: {char_size}\")\n",
    "\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "train_X, train_y = data[:-1], data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(text):\n",
    "    # Create a column vector of zeros with length equal to vocab size\n",
    "    output = np.zeros((char_size, 1))\n",
    "    # Set the index corresponding to the character to 1 (one-hot)\n",
    "    output[char_to_idx[text]] = 1\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350096a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
    "        # Hyperparameters: learning rate, hidden layer size, number of epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Weight matrices and biases for the LSTM gates. The weights expect a\n",
    "        # concatenated vector of previous hidden state and current input, so\n",
    "        # their input dim is `input_size` and output dim is `hidden_size`.\n",
    "        # Forget Gate\n",
    "        self.wf = initWeights(input_size, hidden_size)\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Input Gate\n",
    "        self.wi = initWeights(input_size, hidden_size)\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Candidate Gate (cell update proposal)\n",
    "        self.wc = initWeights(input_size, hidden_size)\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Output Gate\n",
    "        self.wo = initWeights(input_size, hidden_size)\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Final readout layer that maps hidden state to output logits over chars\n",
    "        self.wy = initWeights(hidden_size, output_size)\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "    # Reset Network Memory: clear stored states and gate activations between runs\n",
    "    def reset(self):\n",
    "        self.concat_inputs = {}\n",
    "\n",
    "        # Use -1 key to represent the t-1 state before the sequence starts\n",
    "        self.hidden_states = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.cell_states = {-1: np.zeros((self.hidden_size, 1))}\n",
    "\n",
    "        # Containers to store intermediate values needed for backpropagation\n",
    "        self.activation_outputs = {}\n",
    "        self.candidate_gates = {}\n",
    "        self.output_gates = {}\n",
    "        self.forget_gates = {}\n",
    "        self.input_gates = {}\n",
    "        self.outputs = {}\n",
    "\n",
    "    # Forward Propogation: process a sequence of one-hot encoded inputs\n",
    "    def forward(self, inputs):\n",
    "        self.reset()\n",
    "\n",
    "        outputs = []\n",
    "        for q in range(len(inputs)):\n",
    "            # Concatenate previous hidden state and current input vector\n",
    "            self.concat_inputs[q] = np.concatenate(\n",
    "                (self.hidden_states[q - 1], inputs[q])\n",
    "            )\n",
    "\n",
    "            # Compute gate activations using learned weights and biases\n",
    "            self.forget_gates[q] = sigmoid(\n",
    "                np.dot(self.wf, self.concat_inputs[q]) + self.bf\n",
    "            )\n",
    "            self.input_gates[q] = sigmoid(\n",
    "                np.dot(self.wi, self.concat_inputs[q]) + self.bi\n",
    "            )\n",
    "            self.candidate_gates[q] = tanh(\n",
    "                np.dot(self.wc, self.concat_inputs[q]) + self.bc\n",
    "            )\n",
    "            self.output_gates[q] = sigmoid(\n",
    "                np.dot(self.wo, self.concat_inputs[q]) + self.bo\n",
    "            )\n",
    "\n",
    "            # Update cell state and hidden state according to LSTM equations\n",
    "            self.cell_states[q] = (\n",
    "                self.forget_gates[q] * self.cell_states[q - 1]\n",
    "                + self.input_gates[q] * self.candidate_gates[q]\n",
    "            )\n",
    "            self.hidden_states[q] = self.output_gates[q] * tanh(self.cell_states[q])\n",
    "\n",
    "            # Compute raw output logits (will convert to probabilities later)\n",
    "            outputs += [np.dot(self.wy, self.hidden_states[q]) + self.by]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # Backward Propogation: accumulate gradients through time and update weights\n",
    "    def backward(self, errors, inputs):\n",
    "        d_wf, d_bf = 0, 0\n",
    "        d_wi, d_bi = 0, 0\n",
    "        d_wc, d_bc = 0, 0\n",
    "        d_wo, d_bo = 0, 0\n",
    "        d_wy, d_by = 0, 0\n",
    "\n",
    "        # Initialize gradients for next time-step to zeros\n",
    "        dh_next, dc_next = (\n",
    "            np.zeros_like(self.hidden_states[0]),\n",
    "            np.zeros_like(self.cell_states[0]),\n",
    "        )\n",
    "        for q in reversed(range(len(inputs))):\n",
    "            error = errors[q]\n",
    "\n",
    "            # Gradients for readout (final) layer\n",
    "            d_wy += np.dot(error, self.hidden_states[q].T)\n",
    "            d_by += error\n",
    "\n",
    "            # Backprop into hidden state (sum of readout gradient and next-step gradient)\n",
    "            d_hs = np.dot(self.wy.T, error) + dh_next\n",
    "\n",
    "            # Output gate gradient\n",
    "            d_o = (\n",
    "                tanh(self.cell_states[q])\n",
    "                * d_hs\n",
    "                * sigmoid(self.output_gates[q], derivative=True)\n",
    "            )\n",
    "            d_wo += np.dot(d_o, inputs[q].T)\n",
    "            d_bo += d_o\n",
    "\n",
    "            # Cell state gradient (through tanh nonlinearity)\n",
    "            d_cs = (\n",
    "                tanh(tanh(self.cell_states[q]), derivative=True) * self.output_gates[q],\n",
    "                *d_hs,\n",
    "                +dc_next,\n",
    "            )\n",
    "\n",
    "            # Forget gate gradient\n",
    "            d_f = (\n",
    "                d_cs * self.cell_states[q - 1],\n",
    "                *sigmoid(self.forget_gates[q], derivative=True),\n",
    "            )\n",
    "            d_wf += np.dot(d_f, inputs[q].T)\n",
    "            d_bf += d_f\n",
    "\n",
    "            # Input gate gradient\n",
    "            d_i = (\n",
    "                d_cs * self.candidate_gates[q],\n",
    "                *sigmoid(self.input_gates[q], derivative=True),\n",
    "            )\n",
    "            d_wi += np.dot(d_i, inputs[q].T)\n",
    "            d_bi += d_i\n",
    "\n",
    "            # Candidate gate gradient\n",
    "            d_c = (\n",
    "                d_cs * self.input_gates[q],\n",
    "                *tanh(self.candidate_gates[q], derivative=True),\n",
    "            )\n",
    "            d_wc += np.dot(d_c, inputs[q].T)\n",
    "            d_bc += d_c\n",
    "\n",
    "            # Gradient wrt concatenated inputs (sum of gradients from each gate)\n",
    "            d_z = (\n",
    "                np.dot(self.wf.T, d_f),\n",
    "                +np.dot(self.wi.T, d_i),\n",
    "                +np.dot(self.wc.T, d_c),\n",
    "                +np.dot(self.wo.T, d_o),\n",
    "            )\n",
    "\n",
    "            # Split the gradient back into hidden-state and input parts\n",
    "            dh_next = (d_z[: self.hidden_size, :],)\n",
    "            dc_next = (self.forget_gates[q] * d_cs,)\n",
    "\n",
    "        # Clip gradients to avoid exploding gradients, then apply SGD updates\n",
    "        for d_ in (d_wf, d_bf, d_wi, d_bi, d_wc, d_bc, d_wo, d_bo, d_wy, d_by):\n",
    "            np.clip(d_, -1, 1, out=d_)  # type: ignore\n",
    "\n",
    "        self.wf += d_wf * self.learning_rate\n",
    "        self.bf += d_bf * self.learning_rate\n",
    "\n",
    "        self.wi += d_wi * self.learning_rate\n",
    "        self.bi += d_bi * self.learning_rate\n",
    "\n",
    "        self.wc += d_wc * self.learning_rate\n",
    "        self.bc += d_bc * self.learning_rate\n",
    "\n",
    "        self.wo += d_wo * self.learning_rate\n",
    "        self.bo += d_bo * self.learning_rate\n",
    "\n",
    "        self.wy += d_wy * self.learning_rate\n",
    "        self.by += d_by * self.learning_rate\n",
    "\n",
    "    # Train\n",
    "    def train(self, inputs, labels):\n",
    "        # Convert inputs (characters) to one-hot vectors for the whole sequence\n",
    "        inputs = [oneHotEncode(input) for input in inputs]\n",
    "\n",
    "        for _ in tqdm(range(self.num_epochs)):\n",
    "            # Forward pass to get logits for each time-step\n",
    "            predictions = self.forward(inputs)\n",
    "\n",
    "            # Compute simple cross-entropy style errors to pass to backward\n",
    "            errors = []\n",
    "            for q in range(len(predictions)):\n",
    "                # Start with negative softmax (probabilities) and add 1 to the true class\n",
    "                errors += ([-softmax(predictions[q])],)\n",
    "                errors[-1][char_to_idx[labels[q]]] += 1\n",
    "\n",
    "            # Backpropagate errors through time\n",
    "            self.backward(errors, self.concat_inputs)\n",
    "\n",
    "    # Test\n",
    "    def test(self, inputs, labels):\n",
    "        # Predict the sequence by sampling from the softmax probabilities\n",
    "        accuracy = 0\n",
    "        probabilities = self.forward([oneHotEncode(input) for input in inputs])\n",
    "\n",
    "        output = \"\"\n",
    "        for q in range(len(labels)):\n",
    "            prediction = idx_to_char[\n",
    "                np.random.choice(\n",
    "                    [*range(char_size)], p=softmax(probabilities[q].reshape(-1))\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            output += prediction\n",
    "\n",
    "            if prediction == labels[q]:\n",
    "                accuracy += 1\n",
    "\n",
    "        # Show ground truth and sampled predictions and compute accuracy\n",
    "        print(f\"Ground Truth:\\nt{labels}\\n\")\n",
    "        print(f\"Predictions:\\nt{''.join(output)}\\n\")\n",
    "\n",
    "        print(f\"Accuracy: {round(accuracy * 100 / len(inputs), 2)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41448f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:06<00:00, 15.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth:\n",
      "to be, or not to be, that is the question: whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and by opposing end them. to die—to sleep, no more; and by a sleep to say we end the heart-ache and the thousand natural shocks that flesh is heir to: 'tis a consummation devoutly to be wish'd. to die, to sleep; to sleep, perchance to dream—ay, there's the rub: for in that sleep of death what dreams may come, when we have shuffled off this mortal coil, must give us pause—there's the respect that makes calamity of so long life. for who would bear the whips and scorns of time, th'oppressor's wrong, the proud man's contumely, the pangs of dispriz'd love, the law's delay, the insolence of office, and the spurns that patient merit of th'unworthy takes, when he himself might his quietus make\n",
      "\n",
      "Predictions:\n",
      "to be, or not to be, that is the question: whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and by opposing end them. to die—to sleep, no more— and by a sleep to say we end the heart-ache and the thousand natural shocks that flesh is heir to: 'tis a consummation devoutly to be wish'd. to die, to sleep; to sleep, perchance to dream—ay, there's the rub: for in that sleep of death what dreams may come, when we have shuffled off this mortal coil, must give us pause—there's the respect that makes calamity of so long life. for who would bear the whips and scorns of time, th'oppressor's wrong, the proud man's contumely, the pangs of dispriz'd love, the law's delay, the insolence of office, and the spurns that patient merit of th'unworthy takes, when he himself might his quietus make\n",
      "\n",
      "Accuracy: 99.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Network\n",
    "hidden_size = 25\n",
    "\n",
    "lstm = LSTM(\n",
    "    input_size=char_size + hidden_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=char_size,\n",
    "    num_epochs=1_000,\n",
    "    learning_rate=0.05,\n",
    ")\n",
    "\n",
    "##### Training #####\n",
    "lstm.train(train_X, train_y)\n",
    "\n",
    "##### Testing #####\n",
    "lstm.test(train_X, train_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-network-applications",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
